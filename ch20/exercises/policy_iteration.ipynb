{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03631e9b",
   "metadata": {},
   "source": [
    "# Exercise 1: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0894254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension will automatically load in new code as you edit files, \n",
    "# so you don't need to restart the kernel every time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from ipywidgets import interact\n",
    "\n",
    "from grid_world import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226741d-c1f3-4f18-a492-f3f4fe004c93",
   "metadata": {},
   "source": [
    "In this exercise we will be using a `GridWorld` environment where there are a set of absorbing states that get a reward of 0 and for every other state visited gets a reward of -1. The actions are to go up, down, left, or right, respecting the boundaries of the grid. We can visualize the grid below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89649a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(4, absorbing_states={(0,0), (3,3)})\n",
    "env.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf23abb",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Implement Policy Evaluation\n",
    "First, implement the function `bellman_expectation` which will compute the one-step Bellman update of the value function for a given state and stochastic policy. Then, implement the function `policy_evaluation` to implement the policy evaluation algorithm using the `bellman_expectation` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af065e58-15c7-4d25-a320-67764bb8d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_value(env: GridWorld, state: Tuple[int, int], action: Tuple[int, int], discount: float):\n",
    "    \"\"\"\n",
    "    Compute the action value Q(x, u) for current state and action using one-step lookahead with the\n",
    "    environment's value function.\n",
    "\n",
    "    Args:\n",
    "        env: grid world environment\n",
    "        state: the (x, y) indices that define the location in the grid\n",
    "        action: the movement direction (i.e. (1, 0)) action\n",
    "        discount: discount factor\n",
    "    \"\"\"\n",
    "    next_state, reward = env.step(state, action)\n",
    "    return reward + discount * env.state_value[next_state]\n",
    "\n",
    "def bellman_expectation(env: GridWorld, state: Tuple[int, int], policy_probs: List[float], discount: float) -> float:\n",
    "    \"\"\"\n",
    "    Performs a one-step lookahead and applies the Bellman expectation equation to update the value for\n",
    "    the given state.\n",
    "    \n",
    "    Args:\n",
    "        env: grid world environment\n",
    "        state: the (x, y) indices that define the location in the grid\n",
    "        policy_probs: transition probabilities for each action for the given state\n",
    "        discount: discount factor\n",
    "    Returns:\n",
    "        the new value for the specified state\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    # Hint: Use env.actions.items() and compute_action_value(...)\n",
    "\n",
    "    ###### YOUR CODE END HERE ######\n",
    "\n",
    "def policy_evaluation(env: GridWorld, policy: np.ndarray, eps: float= 1e-2, max_steps: int=1, discount: float=1., in_place: bool=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: grid world environment\n",
    "        policy: a numpy 3-D numpy array, where the first two dimensions identify a state and \n",
    "                the third dimension identifies the actions. The array stores the probability \n",
    "                of taking each action. I.e. policy[i, j, k] is the probability of selection action\n",
    "                k from state (i,j).\n",
    "        eps: convergence threshold\n",
    "        max_steps: max number of iterations of the algorithm\n",
    "        discount: discount factor\n",
    "        in_place: if False, the value table is updated after all the new values have been calculated.\n",
    "             if True the state [i, j] will new already new values for the states [< i, < j]\n",
    "    \"\"\"\n",
    "    state_value_history = [env.state_value]\n",
    "    converged = False\n",
    "    for k in range(max_steps):\n",
    "        values_prev = np.copy(env.state_value)\n",
    "        # cache old values if not in place\n",
    "        values = env.state_value if in_place else np.empty_like(\n",
    "            env.state_value)\n",
    "        ##### YOUR CODE STARTS HERE #####\n",
    "\n",
    "        ###### YOUR CODE END HERE ######\n",
    "        # set the new value table\n",
    "        env.state_value = values\n",
    "        state_value_history.append(env.state_value)\n",
    "        if (np.linalg.norm(values_prev - values, ord=np.inf) < eps):\n",
    "            converged = True\n",
    "            break\n",
    "    return converged, state_value_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357cc30-4af7-4c6f-a685-207f8b7704f1",
   "metadata": {},
   "source": [
    "Now, run the code below to run the policy evaluation algorithm on a simple grid world using a default random policy. Try playing around with a different discount factor to see how that effects the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridWorld\n",
    "env = GridWorld(4, absorbing_states={(0,0), (3,3)})\n",
    "\n",
    "# Define random policy\n",
    "policy = np.ones((*env.state_value.shape, len(env.actions))) * 0.25\n",
    "\n",
    "# Run policy evaluation\n",
    "num_steps = 10\n",
    "discount = 1.0\n",
    "converged, state_value_history = policy_evaluation(env, policy, max_steps=num_steps, discount=discount, in_place=False)\n",
    "print(f\"Converged: {converged}\")\n",
    "@interact(i=(0, num_steps))\n",
    "def f(i=0):\n",
    "    env.render(state_value_history[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e9297",
   "metadata": {},
   "source": [
    "Try playing around with an even larger example world below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1777eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_env = GridWorld(7, absorbing_states={(0,0), (6,6)})\n",
    "policy = np.ones((*large_env.state_value.shape, len(env.actions))) * 0.25\n",
    "converged, _ = policy_evaluation(large_env, policy, max_steps=1000, discount=1., in_place=False)\n",
    "print(f\"Converged: {converged}\")\n",
    "large_env.render(policy=policy);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a24fa3-40dd-4e58-9fe7-1cd0d84905ab",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Implement Policy Iteration\n",
    "Now, implement the `policy_iteration` algorithm below, leveraging the `policy_evaluation` function you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3265d44-1629-4f49-8c77-fdb07f45314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env: GridWorld, eps: float= 1e-2, max_steps: int=1, discount: float=1., β: float=1., in_place: bool=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: grid world environment\n",
    "        eps: convergence threshold\n",
    "        max_steps: max number of iterations of the algorithm\n",
    "        discount: discount factor\n",
    "        β: Boltzmann constant for softmax for stochastic policy definition from action values\n",
    "        in_place: if False, the value table is updated after all the new values have been calculated.\n",
    "             if True the state [i, j] will new already new values for the states [< i, < j]\n",
    "\n",
    "    Returns:\n",
    "        policy: a numpy 3-D numpy array, where the first two dimensions identify a state and \n",
    "                the third dimension identifies the actions. The array stores the probability \n",
    "                of taking each action. I.e. policy[i, j, k] is the probability of selection action\n",
    "                k from state (i,j).\n",
    "    \"\"\"\n",
    "    converged = False\n",
    "    # Reset environment state value function and initialize policy with uniform distribution\n",
    "    env.reset()\n",
    "    policy = np.ones((*env.state_value.shape, len(env.actions))) * 0.25\n",
    "    for k in range(max_steps):\n",
    "        policy_prev = np.copy(policy)\n",
    "        ##### YOUR CODE STARTS HERE #####\n",
    "        # Hint: use softmax() to compute the new policy probabilities\n",
    "        # Update state value function, env.state_value\n",
    "\n",
    "        ###### YOUR CODE END HERE ######\n",
    "        \n",
    "        if (np.linalg.norm((policy - policy_prev).flatten(), ord=np.inf) < eps):\n",
    "            converged = True\n",
    "            break   \n",
    "    return converged, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78ea69-ca6c-4464-9f1b-a1e60043a020",
   "metadata": {},
   "source": [
    "Now, we will revisit the large grid world and see how the optimal value function is different from the value function for the random policy evaluated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6bf7f-a8c5-4798-a07c-a13e20d1569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_env = GridWorld(7, absorbing_states={(0,0), (6,6)})\n",
    "converged, policy = policy_iteration(large_env, max_steps=1000, discount=1., β=5.)\n",
    "print(f\"Converged: {converged}\")\n",
    "large_env.render(policy=policy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ed8ab-2962-4100-a2d3-6f27e7410a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bda33b16be7e844498c7c2d368d72665b4f1d165582b9547ed22a0249a29ca2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

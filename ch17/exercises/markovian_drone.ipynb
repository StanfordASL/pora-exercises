{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03631e9b",
   "metadata": {},
   "source": [
    "# Exercise 3: Markovian Drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1105f76-a4e2-4064-963a-2250535a7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b860681-e7fe-4005-8785-1e43ebd4c7fd",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "Implement the remaining functions of `DroneMDP` to compute the optimal policy via value iteration. Specifically, implement the functions:\n",
    "1. `compute_action_values`: computes the action-values, $Q(x,u) \\coloneqq R(x, u) + \\gamma \\sum_{x'} p(x' \\:|\\: x, u) V(x')$ for all $x \\in \\mathcal{X}$\n",
    "2. `value_iteration`: performs value iteration to compute the optimal value function\n",
    "3. `optimal_policy`: extracts the optimal policy from the optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3ee8e-3679-48e9-9cf0-354d6f3e5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneMDP(object):\n",
    "    def __init__(self, n=20, γ=0.95, σ=10, x_eye=np.array([15, 15]), x_goal=np.array([19, 9])):\n",
    "        self.n = n # grid size\n",
    "        m = 4 # number of control inputs\n",
    "        self.γ = γ\n",
    "        self.x_goal = x_goal\n",
    "\n",
    "        # Storm params\n",
    "        self.x_eye = x_eye\n",
    "        self.σ = σ\n",
    "\n",
    "        # Control input set: 0 = `up`, 1 = `down`, 2 = `left`, 3 = `right`\n",
    "        self.U = np.arange(m)\n",
    "\n",
    "        # Enumerate the transition probabilities for each state and control, from\n",
    "        # each possible current state\n",
    "        # NOTE: `p(x_next; x, u) = P[x[0], x[1], u, x_next[0], x_next[1]]`\n",
    "        self.P = np.zeros((n, n, m, n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                # Start by identifying the set of all possible next states\n",
    "                X_next = np.clip(np.array([\n",
    "                    [i, j + 1],              # up\n",
    "                    [i, j - 1],              # down\n",
    "                    [i - 1, j],              # left\n",
    "                    [i + 1, j],              # right\n",
    "                ]), 0, n - 1)                # clip to grid\n",
    "        \n",
    "                # If the drone is \"caught\" by the storm, it may move in any given\n",
    "                # direction with probability `w*0.25`\n",
    "                x = np.array([i, j])\n",
    "                w = np.exp(-np.sum((x - x_eye)**2) / (2*(σ**2)))\n",
    "                for u in self.U:\n",
    "                    np.add.at(self.P, (i, j, u, X_next[:, 0], X_next[:, 1]), w*0.25)\n",
    "        \n",
    "                # Otherwise, the drone moves in the chosen direction with\n",
    "                # probability `1 - w`\n",
    "                self.P[i, j, self.U, X_next[:, 0], X_next[:, 1]] += 1 - w          \n",
    "\n",
    "        # Define reward matrix, R(x,u) = R[x[0], x[1], u]\n",
    "        self.R = -np.ones((n, n, m))\n",
    "        self.R[x_goal[0], x_goal[1], :m] = 0.\n",
    "\n",
    "    def simulate(self, x_start, π, N=100):\n",
    "        \"\"\"\n",
    "        Simulate the drone MDP given a starting position and a policy.\n",
    "\n",
    "        Parameters:\n",
    "            x_start: starting position\n",
    "            π: policy, a np.ndarray of shape (n, n)\n",
    "            N: number of steps to simulate\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of shape (N, 2) for the trajectory\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(0)  # seed RNG for reproducibility\n",
    "        x = np.zeros((N, 2)).astype(int)\n",
    "        x[0] = x_start\n",
    "        dx = np.array([[0, 1], [0, -1], [-1, 0], [1, 0]])\n",
    "        for k in range(N - 1):\n",
    "            u = π[x[k, 0], x[k, 1]]\n",
    "            w = np.exp(-np.sum((x[k] - self.x_eye)**2) / (2*(self.σ**2)))\n",
    "            P_next = self.P[x[k, 0], x[k, 1], u, :, :]\n",
    "            flat_index = rng.choice(P_next.size, p=P_next.ravel())\n",
    "            x[k+1] = np.unravel_index(flat_index, P_next.shape)\n",
    "        return x\n",
    "\n",
    "    def compute_action_values(self, V):\n",
    "        \"\"\"\n",
    "        Compute array of action-values Q(x,u) = R(x,u) + γ sum_x' p(x'; x,u)V(x')\n",
    "        for all x in the statespace.\n",
    "\n",
    "        Parameters:\n",
    "            V: np.ndarray of shape (n, n) of the value function for each state in the grid\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of shape (n, n, m) for the action-value function, Q\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE STARTS HERE #####\n",
    "        # Hint: Use self.R, self.γ, and self.P\n",
    "\n",
    "        ###### YOUR CODE END HERE ######\n",
    "\n",
    "    def value_iteration(self, eps=1e-4, max_iters=1000):\n",
    "        \"\"\"\n",
    "        Use value iteration to compute the optimal value function, V. We \n",
    "        consider the algorithm to have converged once all values are within\n",
    "        `eps` of the previous iteration value.\n",
    "\n",
    "        Parameters:\n",
    "            eps: tolerance for convergence\n",
    "            max_iters: max number of iterations to run\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of shape (n, n) for the optimal value function, V\n",
    "        \"\"\"\n",
    "        converged = False\n",
    "        V = np.zeros((self.n, self.n))\n",
    "        ##### YOUR CODE STARTS HERE #####\n",
    "        # Note: you need to set `converged` to True if successful\n",
    "\n",
    "        ###### YOUR CODE END HERE ######\n",
    "        if not converged:\n",
    "            raise RuntimeError('Value iteration did not converge!')\n",
    "        return V\n",
    "\n",
    "    def optimal_policy(self, V_star):\n",
    "        \"\"\"\n",
    "        Compute the optimal policy given the optimal value function.\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE STARTS HERE #####\n",
    "\n",
    "        ###### YOUR CODE END HERE ######       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152902d-b271-4fc6-a517-986d01725bf7",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Simulate and Plot\n",
    "Run the code below to compute the optimal policy, simulate it, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daee815-8ba3-4285-8c8f-a046d508b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the optimal value function and policy via value iteration\n",
    "mdp = DroneMDP()\n",
    "V_star = mdp.value_iteration()\n",
    "π_star = mdp.optimal_policy(V_star)\n",
    "\n",
    "# Simulate the drone from a starting position\n",
    "x_start=np.array([0, 19])\n",
    "x = mdp.simulate(x_start, π_star, N=100)\n",
    "\n",
    "# Plot the optimal value function as a heat map\n",
    "fig, ax = plt.subplots(1, 1, dpi=150)\n",
    "im = ax.imshow(V_star.T, origin='lower')\n",
    "plt.colorbar(im, label=r'$V^*\\!(x)$')\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "plt.show()\n",
    "\n",
    "# Plot the optimal policy as a heat map, and overlay the simulated trajectory\n",
    "fig, ax = plt.subplots(1, 1, dpi=150)\n",
    "im = ax.imshow(π_star.T, origin='lower')\n",
    "plt.colorbar(im, label=r'$\\pi^*\\!(x)$')\n",
    "ax.plot(mdp.x_goal[0], mdp.x_goal[1], '*', color='k', markersize=10)\n",
    "ax.plot(mdp.x_eye[0], mdp.x_eye[1], '^', color='tab:orange', markersize=10)\n",
    "ax.plot(x_start[0], x_start[1], 'o', color='tab:green', markersize=10)\n",
    "ax.plot(x[:, 0], x[:, 1], color='tab:red')\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c65abd-c74b-4e3d-9935-46cc0c917150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bda33b16be7e844498c7c2d368d72665b4f1d165582b9547ed22a0249a29ca2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU4mONuMOIjr"
   },
   "source": [
    "# Exercise 1: Object Detection\n",
    "\n",
    "In this problem, we will explore some of the features that torchvision (a pytorch library) offers for visualizing images, performing object detection with pretrained CNN models, and extracting and plotting detected bounding boxes. We'll start by downloading a set of images that we'll use for detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection import fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    "%matplotlib inline\n",
    "\n",
    "imagefolder = os.path.join(os.getcwd(), 'images')\n",
    "\n",
    "def show(img: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Plots a torch image Tensor.\n",
    "\n",
    "    Args:\n",
    "        img: (3, H, W) torch Tensor of an image.\n",
    "    \"\"\"\n",
    "    img = img.detach()\n",
    "    img = F.to_pil_image(img)\n",
    "    plt.imshow(np.asarray(img))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Object detection with pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-U69cwZOIju"
   },
   "source": [
    "In the first part of this problem, we will perform object detection using a pretrained torchvision model. By *pretrained*, we mean that the weights of the model have already undergone training, and will not be changing over the course of this problem. We will be using a pretrained `fcos_resnet50_fpn()` model or a pretrained YOLOv5 model. \n",
    "Your task is to implement:\n",
    "1. `fcos_resnet50`: evaluates an image using the model described [here](https://pytorch.org/vision/0.16/models/generated/torchvision.models.detection.fcos_resnet50_fpn.html#torchvision.models.detection.fcos_resnet50_fpn). See the documentation for details on how to load and evaluate the model.\n",
    "2. `yolo_v5`: evaluates an image using the YOLOv5 model described [here](https://github.com/ultralytics/yolov5). See the documentation for details on how to load and evalute the model.\n",
    "\n",
    "Both functions will take an image as input and output `boxes`, `scores`, and `labels` for each object detected in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BlksSNQE9wR",
    "outputId": "6a338c70-2a80-4472-d195-6c2d7977ccd0"
   },
   "outputs": [],
   "source": [
    "def fcos_resnet50(image: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate the fcos_resnet50_fpn model with the given image.\n",
    "\n",
    "    Params:\n",
    "        image: image to evaluate\n",
    "\n",
    "    Returns:\n",
    "        boxes: box detections\n",
    "        scores: scores for each detection\n",
    "        labels: labels for each detection\n",
    "    \"\"\"\n",
    "    weights = FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "    ########## Code starts here ##########\n",
    "    # Load the pretrained FCOS model\n",
    "    # Load preprocessing transforms from the weights\n",
    "    # Load and preprocess an image\n",
    "    # Run inference\n",
    "    \n",
    "    ########## Code ends here ############\n",
    "\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_v5(image: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Evaluate the YOLOv5 model with the given image.\n",
    "\n",
    "    Params:\n",
    "        image: image to evaluate\n",
    "\n",
    "    Returns:\n",
    "        boxes: box detections\n",
    "        scores: scores for each detection\n",
    "        labels: labels for each detection\n",
    "    \"\"\"\n",
    "    # YOLOv5 can be loaded directly from torch.hub\n",
    "    # This will download the model if not already cached\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "    \n",
    "    ########## Code starts here ##########\n",
    "    # Hints:\n",
    "    # 1. YOLOv5 models can be loaded using torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "    #    Available models: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x (n=nano, s=small, m=medium, l=large, x=extra large)\n",
    "    # 2. YOLOv5 models are in evaluation mode by default when loaded with pretrained=True\n",
    "    # 3. YOLOv5 can handle single images directly (no need for manual batching)\n",
    "    # 4. YOLOv5 handles normalization internally - you can pass PIL Images, numpy arrays, or file paths directly\n",
    "    # 5. The model returns a Results object with detection information\n",
    "    \n",
    "    ########## Code ends here ############\n",
    "\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Visualize Object Detections\n",
    "Implement the function `draw_result` which produces an image with an overlay of the bounding box detections, their labels, and their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_result(\n",
    "    image: torch.Tensor,\n",
    "    boxes: torch.Tensor,\n",
    "    labels: list[str],\n",
    "    scores: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Draw bounding box visualization over top of the raw image.\n",
    "\n",
    "    Args:\n",
    "        image: (3, H, W) torch Tensor of an image with values 0-255.\n",
    "        boxes: (N, 4) torch Tensor of detected bounding boxes in xyxy format.\n",
    "        labels: (N,) length list of string label names\n",
    "        scores: (N,) torch Tensor of confidence scores.\n",
    "\n",
    "    Returns:\n",
    "        img_with_bbox: (3, H, W) torch Tensor of an image with bounding boxes drawn atop.\n",
    "    \"\"\"\n",
    "    ########## Code starts here ##########\n",
    "    # Hints:\n",
    "    # 1. This function is small, the solution is only a couple of lines.\n",
    "    # 2. See documentation for draw_bounding_boxes here:\n",
    "    #    https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html\n",
    "    # 3. You need to create label strings that combine class names and confidence scores\n",
    "    # 4. Format strings like f\"{label}: {score:.2f}\" work well for labels\n",
    "\n",
    "    ########## Code ends here ##########\n",
    "\n",
    "    return img_with_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below with a selected image and model to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Select an image on which to perform object detection.\n",
    "imagepath = os.path.join(imagefolder, 'airport.jpeg')\n",
    "# imagepath = os.path.join(imagefolder, 'cars.jpeg')\n",
    "# imagepath = os.path.join(imagefolder, 'dog_park.jpeg')\n",
    "\n",
    "# Load image\n",
    "image = read_image(imagepath)\n",
    "show(image)\n",
    "\n",
    "# NOTE: Select a model to explore\n",
    "boxes, scores, labels = fcos_resnet50(image)\n",
    "# boxes, scores, labels = yolo_v5(image)\n",
    "\n",
    "# Show overlayed image with detections\n",
    "show(draw_result(image, boxes, labels, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Filter Low Confidence Detections\n",
    "Implement the function `filter` which produces a filtered set of object detections based on the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(boxes: torch.Tensor, labels: list[str], scores: torch.Tensor, threshold: float):\n",
    "    \"\"\"\n",
    "    Filter the detections based on scores and a given threshold.\n",
    "\n",
    "    Params:\n",
    "        boxes: box detections\n",
    "        labels: list of label strings\n",
    "        scores: scores for each detection\n",
    "        threshold: threshold for the score\n",
    "\n",
    "    Returns:\n",
    "        filtered boxes\n",
    "        filtered labels\n",
    "        filtered scores\n",
    "    \"\"\"\n",
    "    ########## Code starts here ##########\n",
    "    # Hints for threshold exploration:\n",
    "    # 1. Can define a boolean mask by: mask = scores >= threshold\n",
    "    # 2. Can directly mask torch.Tensor with x[mask]\n",
    "    mask = scores >= threshold\n",
    "    filtered_labels = [label for label, m in zip(labels, mask) if m]\n",
    "    return boxes[mask], filtered_labels, scores[mask]\n",
    "    ########## Code ends here ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to show the filtered object detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(draw_result(image, *filter(boxes, labels, scores, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

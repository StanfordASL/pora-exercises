{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5ddefc",
   "metadata": {},
   "source": [
    "# Exercise 2: Advantage Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a178ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import cv2\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "from a2c_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e69f6b-3b73-4400-85f7-7770c54f785d",
   "metadata": {},
   "source": [
    "The Lunar Lander environment is a playground for RL defined in the [Gymnasium library](https://gymnasium.farama.org/environments/box2d/lunar_lander/). In this exercise, we will use the advantage actor-critic (A2C) method to learn a policy to control the lunar lander in an optimal way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b4b20-ec37-47c3-9950-f38cd907e2a4",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Define the Policy\n",
    "In the first part of this exercise, implement the `Policy.__call__` method to define the model that we will train to control the lunar lander. This model will output a `MultivariateNormalDiag` object that defines a multivariate normal distribution over the action space (see the definition in `a2c_utils.py`, and also an estimate of the value function for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82332cba-dc28-487b-85c5-3703309fde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(eqx.Module):\n",
    "    # Define a model to represent the actor and critic. \n",
    "    trunk_layers: list\n",
    "    action_mean_head: eqx.Module\n",
    "    action_std_head: eqx.Module\n",
    "    value_head: eqx.Module\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, key):\n",
    "        # `PRNGKey`s for initializing NN layers.\n",
    "        keys = jax.random.split(key, 5)\n",
    "        \n",
    "        # Embedding layers.\n",
    "        self.trunk_layers = [\n",
    "            eqx.nn.Linear(state_dim, 128, key=keys[0]),\n",
    "            eqx.nn.Linear(128, 128, key=keys[1]),\n",
    "        ]\n",
    "        \n",
    "        # Actor's layers.\n",
    "        self.action_mean_head = eqx.nn.Linear(128, action_dim, key=keys[2])\n",
    "        self.action_std_head = eqx.nn.Linear(128, action_dim, key=keys[3])\n",
    "        \n",
    "        # Critic's layers.\n",
    "        self.value_head = eqx.nn.Linear(128, 1, key=keys[4])\n",
    "        \n",
    "    @jax.jit\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Evaluate the policy at the input `x`.\n",
    "\n",
    "        Args:\n",
    "            x: input to evaluate\n",
    "        Returns:\n",
    "            A tuple with the first element being a MultivariateNormalDiag representing\n",
    "            the policy for the input and the second element being the float value function\n",
    "            estimate for the input.\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE STARTS HERE #####\n",
    "        # Create a model with two linear embedding layers with ReLU activations\n",
    "        # that feed into the action_mean head, standard deviation head, and value head.\n",
    "        # Hint: use jax.nn.softplus for the standard deviation head.\n",
    "        # Hint: make sure to use all of the Policy member variables defined above\n",
    "        \n",
    "        ###### YOUR CODE END HERE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a251c0c5-98c5-408d-94c8-8bf6c8a7e0e9",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Compute Episode Returns and Training Loss Function\n",
    "Implement the following functions:\n",
    "1. `compute_returns`: a function to compute the discounted tail returns for an episode\n",
    "2. `train_loss_for_epsiode`: a function to compute the training loss for an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d680c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, discount_factor):\n",
    "    \"\"\"\n",
    "    Compute the discounted returns from a sequence of rewards:\n",
    "      rewards = [r_0, r_1, ..., r_{T-1}].\n",
    "\n",
    "    Specifically, compute the list of discounted tail returns, [G_0, G_1, ..., G_{T-1}]\n",
    "    where:\n",
    "      G_t = sum_{k=t}^{T-1} Î³^{k-t} r_k\n",
    "\n",
    "    Args:\n",
    "        rewards: array of rewards, [r_0, r_1, ..., r_{T-1}]\n",
    "        discount_factor: temporal discount factor for rewards\n",
    "    Returns:\n",
    "        Array of returns, [G_0, G_1, ..., G_{T-1}]\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    \n",
    "    ###### YOUR CODE END HERE ######\n",
    "\n",
    "def train_loss_for_epsiode(policy, states, actions, returns, num_steps):\n",
    "    \"\"\"\n",
    "    Compute the loss function for the given episode data. Uses Monte Carlo estimates\n",
    "    of the value function via the `returns` input to compute the advantage function.\n",
    "\n",
    "    Args:\n",
    "        policy: policy model\n",
    "        states: array of states for the episode\n",
    "        actions: array of actions taken in the episode\n",
    "        returns: array of discounted tail returns for the episode\n",
    "        num_steps: number of steps in the episode\n",
    "    Returns:\n",
    "        Float representing the actor loss + critic loss\n",
    "    \"\"\"\n",
    "    mask = jnp.arange(len(states)) < num_steps\n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    # Hint: use jax.vmap on policy\n",
    "    # Hint: when you compute the actor loss, use jax.lax.stop_gradient(advantages)\n",
    "    # to make sure the gradients of the critical model are not computed in the actor\n",
    "    # loss.\n",
    "    # Hint: use the `mask` variable on the computed advantages to since the states,\n",
    "    # actions, and returns are padded but only the first num_steps values are valid.\n",
    "    \n",
    "    ###### YOUR CODE END HERE ######\n",
    "\n",
    "@jax.jit\n",
    "def train_step_for_episode(opt_state, policy, states, actions, returns, num_steps):\n",
    "    \"\"\"\n",
    "    Use the function `train_loss_for_epsiode` to update the model parameters.\n",
    "    \"\"\"\n",
    "    grads = jax.grad(train_loss_for_epsiode)(policy, states, actions, returns, num_steps)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    policy = optax.apply_updates(policy, updates)\n",
    "    return opt_state, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4505e045-d384-4e90-b8bc-43b37e04da3f",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Train Model\n",
    "Run the code below to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ce36e-f5d6-4f44-9b9e-f48fbac42e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "discount_factor = 0.99 # Discount factor for computing tail returns.\n",
    "ema_factor = 0.99 # Exponential moving average for standardizing returns.\n",
    "key = jax.random.PRNGKey(0) # Random seed for NN initialization, action sampling.\n",
    "max_steps = 600 # Max number of steps in a given episode\n",
    "num_episodes = 300 # Number of episodes to train over\n",
    "num_renders = 4 # Render video of the lunar lander this many times\n",
    "\n",
    "# Set display\n",
    "Display(visible=False, size=(1400, 900)).start()\n",
    "\n",
    "# Define lunar lander environment\n",
    "lunar_lander = LunarLander()\n",
    "\n",
    "# Define policy object\n",
    "key, policy_key = jax.random.split(key)\n",
    "policy = Policy(lunar_lander.state_dim, lunar_lander.action_dim, policy_key)\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(policy)\n",
    "\n",
    "# Train\n",
    "episodic_rewards_ema = [] # Exponential moving average of episodic rewards.\n",
    "return_ema = None  # Exponential moving average of returns (i.e., critic targets).\n",
    "return_emv = None  # Exponential moving variance of returns (i.e., critic targets).\n",
    "render_interval = int(num_episodes / num_renders) \n",
    "for i_episode in range(num_episodes + 1):\n",
    "    video_filename = f\"{lunar_lander.video_file_directory}/lunar_lander_episode_{i_episode}.mp4\" \\\n",
    "                        if i_episode % render_interval == 0 else None\n",
    "    \n",
    "    # Reset environment at the start of each episode, and clear accumulators.\n",
    "    state = lunar_lander.env.reset()[0]\n",
    "    episodic_reward = 0\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    # Sample a trajectory (episode) according to our stochastic policy/environment dynamics.\n",
    "    for t in range(max_steps):\n",
    "        states.append(state)\n",
    "        action_distribution, _ = policy(state)\n",
    "        key, sample_key = jax.random.split(key)\n",
    "        action = np.array(action_distribution.sample(sample_key))  # Leave JAX to interact with gym.\n",
    "        state, reward, done, _, _ = lunar_lander.env.step(action)\n",
    "        episodic_reward += reward\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # Render video at specified interval of episodes\n",
    "        if video_filename is not None:\n",
    "            if t == 0:\n",
    "                video = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*\"mp4v\"), 50, (600, 400))\n",
    "            video.write(lunar_lander.env.render())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if video_filename is not None:\n",
    "        video.release()\n",
    "        display_local_video_in_notebook(video_filename)\n",
    "\n",
    "    # Compute (standardized) tail returns for the episode and update moving averages.\n",
    "    returns = compute_returns(rewards, discount_factor)\n",
    "    if i_episode == 0:\n",
    "        episodic_rewards_ema.append(episodic_reward)\n",
    "        return_ema = returns.mean()\n",
    "        return_emv = returns.var()\n",
    "    else:\n",
    "        episodic_rewards_ema.append(0.95 * episodic_rewards_ema[-1] + (1 - 0.95) * episodic_reward)\n",
    "        return_ema = ema_factor * return_ema + (1 - ema_factor) * returns.mean()\n",
    "        return_emv = ema_factor * (return_emv + (1 - ema_factor) * np.mean((returns - return_ema)**2))\n",
    "    # Note: standardizing returns using moving population statistics is reminiscent of batch normalization.\n",
    "    standardized_returns = (returns - return_ema) / (np.sqrt(return_emv) + 1e-6)\n",
    "\n",
    "    # Run a train step based on the episode's data.\n",
    "    num_steps = len(states)\n",
    "    # JAX prefers all arrays to be the same shape, so we pad to the batch size.\n",
    "    opt_state, policy = train_step_for_episode(\n",
    "        opt_state,\n",
    "        policy,\n",
    "        np.pad(states, ((0, max_steps - num_steps), (0, 0))),\n",
    "        np.pad(actions, ((0, max_steps - num_steps), (0, 0))),\n",
    "        np.pad(standardized_returns, ((0, max_steps - num_steps),)),\n",
    "        num_steps,\n",
    "    )\n",
    "\n",
    "    # Periodically log results.\n",
    "    if i_episode % 10 == 0:\n",
    "        print(\n",
    "            f\"Episode {i_episode}\\tLast reward: {episodic_reward:.2f}\\tMoving average reward: {episodic_rewards_ema[-1]:.2f}\"\n",
    "        )\n",
    "\n",
    "# Plot EMA rewards\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(episodic_rewards_ema)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward (exponential moving average)')\n",
    "plt.title('Total Reward per Episode')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

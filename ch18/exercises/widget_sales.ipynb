{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03631e9b",
   "metadata": {},
   "source": [
    "# Exercise 1: Q-learning Widget Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae3e18-be1c-4007-bffc-550d3b1fa82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651e244-a96b-486f-a353-87af6ed2af55",
   "metadata": {},
   "source": [
    "Below we define a simple `WidgetShop` class that defines the true shop dynamics and demand distributions, as well as the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f11298-d5ea-49a2-a02e-23289044d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WidgetShop:\n",
    "    def __init__(self):\n",
    "        self.X = np.array([0, 1, 2, 3, 4, 5]) # state space\n",
    "        self.U = np.array([0, 2, 4]) # action space\n",
    "        self.D = np.array([0, 1, 2, 3, 4]) # demand space\n",
    "        self.P = np.array([0.1, 0.3, 0.3, 0.2, 0.1]) # daily demand distribution\n",
    "\n",
    "    def action_idx(self, u: int):\n",
    "        \"\"\"\n",
    "        Maps an action in self.U to an index.\n",
    "        \"\"\"\n",
    "        return int(u/2)\n",
    "\n",
    "    def step(self, x: int, u: int, d: int) -> int:\n",
    "        \"\"\"\n",
    "        Compute the next state given the current state, action, and demand.\n",
    "        \"\"\"\n",
    "        return np.clip(x + u - d, 0, 5)\n",
    "\n",
    "\n",
    "    def reward(self, x: int, u: int, d: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward given the current state, action, and demand.\n",
    "        \"\"\"\n",
    "        price = 1.2\n",
    "        cost_rent = 1.\n",
    "        cost_storage = 0.05*x\n",
    "        cost_order = np.sqrt(u)\n",
    "        r = price*np.minimum(x + u, d) - cost_rent - cost_storage - cost_order\n",
    "        return r\n",
    "    \n",
    "    def simulate(self,\n",
    "                 policy: callable,\n",
    "                 T: int,\n",
    "                 x0: int = 5) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Simulate widget sales for a given policy.\n",
    "\n",
    "        Parameters:\n",
    "            policy: policy to simulate, callable as policy(x)\n",
    "            T: number of time steps to simulate\n",
    "            x0: initial inventory\n",
    "        \"\"\"\n",
    "        x = np.zeros(T + 1)  # states\n",
    "        u = np.zeros(T)      # actions\n",
    "        r = np.zeros(T)      # rewards\n",
    "        x[0] = x0            # initial state\n",
    "        rng = np.random.default_rng(0) # for reproducibility\n",
    "        for t in range(T):\n",
    "            # Sample demand\n",
    "            d = rng.choice(self.D, p=self.P)\n",
    "    \n",
    "            # Record action, reward, and next state\n",
    "            u[t] = policy(x[t])\n",
    "            r[t] = self.reward(x[t], u[t], d)\n",
    "            x[t+1] = self.step(x[t], u[t], d)\n",
    "    \n",
    "        return x, u, r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab96db-f346-427d-b4a3-2229f72df9ec",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Model-free Q-learning\n",
    "In this first exercise, you will use a dataset of past widget sales to learn tabulated Q-values without the use of a model of the environment. Specifically, in the code below update the array `Q` using Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c65abd-c74b-4e3d-9935-46cc0c917150",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop = WidgetShop()\n",
    "\n",
    "# Generate historical data with a uniformly random policy\n",
    "log = {}\n",
    "T = 3 * 365\n",
    "rng = np.random.default_rng(0) # for reproducibility\n",
    "random_policy = lambda x: rng.choice(shop.U)\n",
    "log['x'], log['u'], log['r'] = shop.simulate(random_policy, T)\n",
    "\n",
    "# Q-learning\n",
    "γ = 0.95                   # discount factor\n",
    "α = 1e-2                   # learning rate\n",
    "num_epochs = 5 * int(1/α)  # number of epochs\n",
    "\n",
    "Q = np.zeros((shop.X.size, shop.U.size))\n",
    "Q_epoch = np.zeros((num_epochs + 1, shop.X.size, shop.U.size))\n",
    "\n",
    "for k in range(1, num_epochs + 1):\n",
    "    # Shuffle transition tuple indices\n",
    "    shuffled_indices = rng.permutation(T)\n",
    "\n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    # Hint: You can use shop.action_idx() to convert the action to a tabular index\n",
    "    # Do a Q-update for each transition tuple\n",
    "\n",
    "    ###### YOUR CODE END HERE ######\n",
    "\n",
    "    # Record Q-values for this epoch\n",
    "    Q_epoch[k] = Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3e394-af01-416a-b325-5f1b52d79f20",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Value Iteration\n",
    "We will now leverage the model of the widget shop supply and demand to compute the Q-value using value iteration. Finish the code below to update the array `Q_vi` using value iteration. Then, run the code to produce plots comparing the Q-values from value iteration vs the values computed above using the model-free data approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730e25f-aa55-4a1d-ba65-b3de3713c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "converged = False\n",
    "eps = 1e-4\n",
    "max_iters = 500\n",
    "Q_vi = np.zeros((shop.X.size, shop.U.size))\n",
    "Q_vi_prev = np.full(Q_vi.shape, np.inf)\n",
    "\n",
    "for k in range(max_iters):\n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "\n",
    "    ###### YOUR CODE END HERE ######\n",
    "\n",
    "    if np.max(np.abs(Q_vi - Q_vi_prev)) < eps:\n",
    "        converged = True\n",
    "        print('Value iteration converged after {} iterations.'.format(k))\n",
    "        break\n",
    "    else:\n",
    "        np.copyto(Q_vi_prev, Q_vi)\n",
    "\n",
    "if not converged:\n",
    "    raise RuntimeError('Value iteration did not converge!')\n",
    "\n",
    "# Plot Q-values for each epoch\n",
    "fig, axes = plt.subplots(2, shop.X.size//2, figsize=(12, 6),\n",
    "                         sharex=True, sharey=True, dpi=150)\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    for j in range(shop.U.size):\n",
    "        plot = ax.plot(Q_epoch[:, i, j], label='$u = {}$'.format(shop.U[j]))\n",
    "        ax.axhline(Q_vi[i, j], linestyle='--', color=plot[0].get_color())\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.set_title(r'$x = {}$'.format(shop.X[i]))\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel('epoch')\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel('$Q(x,u)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ec568-7016-46c1-8fc5-e9fd2ed58a3a",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Simulate Q-learning and Value Iteration Optimal Policies\n",
    "Write code below to compute:\n",
    "1. `π_ql(x)` and `π_vi(x)`: lambda functions, using the corresponding Q-value arrays computed above.\n",
    "2. `r_ql` and `r_vi`: arrays of rewards from simulateing the shop with the above policies for 5 years.\n",
    "1. `profit_ql` and `profit_vi`: arrays of cumulative profits for each day over 5 years (can use `np.cumsum`)\n",
    "\n",
    "What do you notice about the cumulative profits from the two methods? Why might there be a difference between the non-discount and discounted cumulative profits? Try playing around with the discount factor to see how this affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06951fbf-8e1b-4c44-bd06-431cf7847c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate optimal policies for 5 years\n",
    "T = 5 * 365\n",
    "\n",
    "##### YOUR CODE STARTS HERE #####\n",
    "\n",
    "###### YOUR CODE END HERE ######\n",
    "\n",
    "print('Optimal policy (Q-learning):     ', [int(π_ql(x)) for x in shop.X])\n",
    "print('Optimal policy (value iteration):', [int(π_vi(x)) for x in shop.X])\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(profit_ql, label=r'$Q$-learning')\n",
    "ax.plot(profit_vi, label=r'value iteration')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlabel(r'day $t$')\n",
    "ax.set_ylabel(r'cumulative profit $\\sum_{k=0}^t r_k$')\n",
    "plt.show()\n",
    "\n",
    "# Repeat with discounted profits\n",
    "profit_ql_discounted = np.cumsum((γ**np.arange(T)) * r_ql)\n",
    "profit_vi_discounted = np.cumsum((γ**np.arange(T)) * r_vi)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(profit_ql_discounted, label=r'$Q$-learning')\n",
    "ax.plot(profit_vi_discounted, label=r'value iteration')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlabel(r'day $t$')\n",
    "ax.set_ylabel(r'cumulative discounted profit $\\sum_{k=0}^t \\gamma^k r_k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9e740-0882-4397-924f-c8dda00217fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bda33b16be7e844498c7c2d368d72665b4f1d165582b9547ed22a0249a29ca2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
